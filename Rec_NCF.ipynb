{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4b0088f-c8c1-4df9-893f-19582847a518",
   "metadata": {
    "id": "-z-3KRKDj5R6"
   },
   "source": [
    "**CODE WITH WITH ADDED ATTENTION LAYER, MATRIX FACTORIZATION (SINGULAR VALUE DECOMPOSITION) AND NEUTRAL COLLABORATIVE FILTERING(NCF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0c1e97-5e8f-4d02-8b5f-8ad708f4d52e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from fastFM import sgd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Loading the train and test datasets\n",
    "train_file = 'train_inter.tsv'\n",
    "test_file = 'test_inter.tsv'\n",
    "\n",
    "# Loading data from corresponding TSV files\n",
    "train_data = pd.read_csv(train_file, sep='\\t', error_bad_lines=False)\n",
    "test_data = pd.read_csv(test_file, sep='\\t', error_bad_lines=False)\n",
    "\n",
    "# Data preprocessing and mapping to integer indices for metadata\n",
    "train_data = train_data.dropna(subset=['rec_his', 'src_his', 'ts', 'label'])\n",
    "test_data = test_data.dropna(subset=['rec_his', 'src_his', 'ts', 'label'])\n",
    "\n",
    "X_train = train_data[['rec_his', 'src_his', 'ts']].values\n",
    "y_train = train_data['label'].values\n",
    "\n",
    "X_test = test_data[['rec_his', 'src_his', 'ts']].values\n",
    "y_test = test_data['label'].values\n",
    "\n",
    "# Defining the attention layer\n",
    "class AttentionLayer(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W_q = self.add_weight(name='W_q',\n",
    "                                  shape=(input_shape[-1], input_shape[-1]),\n",
    "                                  initializer='uniform',\n",
    "                                  trainable=True)\n",
    "        self.W_k = self.add_weight(name='W_k',\n",
    "                                  shape=(input_shape[-1], input_shape[-1]),\n",
    "                                  initializer='uniform',\n",
    "                                  trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        q = tf.matmul(x, self.W_q)\n",
    "        k = tf.matmul(x, self.W_k, transpose_b=True)\n",
    "        v = x\n",
    "\n",
    "        attn_scores = tf.matmul(q, k, transpose_b=True)\n",
    "        attn_scores = tf.nn.softmax(attn_scores)\n",
    "        output = tf.matmul(attn_scores, v)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Matrix Factorization (SVD)\n",
    "svd = TruncatedSVD(n_components=1, random_state=42)  # Adjust the number of components\n",
    "X_train_svd = svd.fit_transform(X_train)\n",
    "X_test_svd = svd.transform(X_test)\n",
    "\n",
    "# Determining the number of unique users and items in the dataset\n",
    "num_users = len(pd.unique(train_data['u_id'].append(test_data['u_id'])))\n",
    "num_items = len(pd.unique(train_data['i_id'].append(test_data['i_id'])))\n",
    "\n",
    "# Neural Collaborative Filtering (NCF)\n",
    "user_input = keras.layers.Input(shape=(1,))\n",
    "item_input = keras.layers.Input(shape=(1,))\n",
    "user_embedding_layer = keras.layers.Embedding(input_dim=num_users, output_dim=50, input_length=1)\n",
    "item_embedding_layer = keras.layers.Embedding(input_dim=num_items, output_dim=50, input_length=1)\n",
    "user_embedding = user_embedding_layer(user_input)\n",
    "item_embedding = item_embedding_layer(item_input)\n",
    "merged = keras.layers.Dot(axes=1)([user_embedding, item_embedding])\n",
    "ncf_output = keras.layers.Dense(1, activation='sigmoid')(merged)\n",
    "ncf_model = keras.Model(inputs=[user_input, item_input], outputs=ncf_output)\n",
    "\n",
    "# Building the recommender model with class weights and L2 regularization\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    AttentionLayer(),\n",
    "    keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Configuring the optimizer with a learning rate scheduler\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100, decay_rate=0.01, staircase=True\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipvalue=1.0)\n",
    "\n",
    "# Compiling the model with binary cross-entropy loss and class weights\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Implementing early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=6, restore_best_weights=True)\n",
    "\n",
    "# Training the model with class weights\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Binary crossentropy: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "integer_predictions = [1 if prediction >= 0.5 else 0 for prediction in y_pred]\n",
    "\n",
    "# Calculating precision, recall, and F1 score\n",
    "precision = precision_score(y_test, integer_predictions)\n",
    "recall = recall_score(y_test, integer_predictions)\n",
    "f1 = f1_score(y_test, integer_predictions)\n",
    "\n",
    "\n",
    "# Recommendations for a user as per the user history rec_his\n",
    "user = X_test[0:1]\n",
    "predictions = model.predict(user)\n",
    "user_integer_predictions = [1 if prediction >= 0.5 else 0 for prediction in predictions]\n",
    "print(f\"Recommendations for the user: {user_integer_predictions}\")\n",
    "\n",
    "\n",
    "# Display the metrics table\n",
    "metrics_data = {'Metric': ['Precision', 'Recall', 'F1 Score'],\n",
    "                'Value': [precision, recall, f1]}\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "print(metrics_df)\n",
    "print(\"\\n\")\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=train_data, x='rec_his', kde=True, color='blue', label='Recommendation')\n",
    "sns.histplot(data=train_data, x='src_his', kde=True, color='orange', label='Search')\n",
    "plt.xticks(range(0, 301, 50))\n",
    "plt.xlim(0, 300)\n",
    "plt.title('Recommendation vs Search')\n",
    "plt.xlabel('User Interaction')\n",
    "plt.ylabel('User Count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172cf213-3ef8-466c-acc7-ee7ed6c96dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
